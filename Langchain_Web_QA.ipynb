{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tritab/qa/blob/main/Langchain_Web_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icMXn63poSKg"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain requests openai transformers faiss-cpu sentence_transformers"
      ],
      "metadata": {
        "id": "xD55ot49HXJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "OPENAI_API_KEY = getpass('Enter your OpenAI key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDDLMNP6ChMF",
        "outputId": "69d6cc09-bc47-4f94-a170-2ce64b62c852"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "c_Fa6malpPy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "import requests\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline\n"
      ],
      "metadata": {
        "id": "FUUgX0HCozbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from time import sleep\n",
        "import pickle\n",
        "\n",
        "def get_page_text(url, depth=2, visited_links=None, max_links=10, cache=None, timeout=3, user_agent=None):\n",
        "    \"\"\"\n",
        "    Recursively follow links on a webpage and return a list of documents of subsequent found pages.\n",
        "    :param url: The URL of the webpage to scrape\n",
        "    :param depth: The number of levels deep to recursively follow links. Default is 2.\n",
        "    :param visited_links: A dictionary or list of links that have already been visited to prevent revisiting links\n",
        "    :param max_links: The maximum number of links to follow. Default is 50.\n",
        "    :param cache: A cache of links and their corresponding documents to prevent unnecessary web requests\n",
        "    :param timeout: Number of seconds to wait before timing out a request. Default is 5.\n",
        "    :param user_agent: The User Agent string to use for requests. Default is None.\n",
        "    \"\"\"\n",
        "    # Initialize the visited links set if not provided\n",
        "    if visited_links is None:\n",
        "        visited_links = {}\n",
        "    if cache is None:\n",
        "        cache = {}\n",
        "    # Extract the root domain from the URL\n",
        "    parsed_uri = urlparse(url)\n",
        "    root_domain = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri)\n",
        "    # Check if the link has already been visited\n",
        "    if url in visited_links:\n",
        "        print(\"Hit in visited_links set: \", url)\n",
        "        return None\n",
        "    # Check if the link is in the cache\n",
        "    if url in cache:\n",
        "        print(\"Hit in cache set: \", url)\n",
        "        return cache[url]\n",
        "    # Check for relative paths, fragments, and mailto links\n",
        "    if not parsed_uri.netloc:\n",
        "        print(\"Invalid URL: \", url)\n",
        "        return None\n",
        "    visited_links[url] = True\n",
        "    # Send a GET request to the URL and handle common errors\n",
        "    try:\n",
        "        headers = {'User-Agent': user_agent or 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36','Accept-Language': 'en-US,en;q=0.5'}\n",
        "        print(\"Retrieving: \", url, headers)\n",
        "        page = requests.get(url, headers=headers)\n",
        "        page.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error retrieving the webpage {url}: {str(e)}\")\n",
        "        return None\n",
        "    # parse the HTML and extract the text\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "    text = soup.get_text()\n",
        "    # Add the link and its corresponding document to the cache\n",
        "    cache[url] = Document(page_content=text, metadata={\"source\": url})\n",
        "    with open('scrape_cache.pickle', 'wb') as handle:\n",
        "        pickle.dump(cache, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    # Check if we have reached the maximum depth or maximum number of links to follow\n",
        "    if depth <= 0 or max_links <= 0:\n",
        "        return cache[url]\n",
        "    # Follow links on the webpage\n",
        "    links = []\n",
        "    for link in soup.find_all('a'):\n",
        "        href = link.get('href')\n",
        "        # Only follow links that are on the same root domain\n",
        "        if href and root_domain in href:\n",
        "            links.append(href)\n",
        "    # Follow the links recursively and space out the requests to avoid throttling\n",
        "    for link in links:\n",
        "        sleep(timeout)\n",
        "        doc = get_page_text(link, depth-1, visited_links, max_links-1, cache, timeout, user_agent)\n",
        "        if doc:\n",
        "            cache[link] = doc\n",
        "    return cache\n"
      ],
      "metadata": {
        "id": "tjjWZK3s-0xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# There is a bug if depth is set >0 due to the dict type"
      ],
      "metadata": {
        "id": "fUwerr_aTNH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sources = [\n",
        "    # get_page_text(\"https://www.guildeducation.com/solutions/\", depth=0),\n",
        "    # get_page_text(\"https://www.guildeducation.com/leadership/\", depth=0),\n",
        "    # get_page_text(\"https://blog.guildeducation.com/\", depth=1),\n",
        "    # get_page_text(\"https://www.guildeducation.com/terms/\", depth=0),\n",
        "    get_page_text(url=\"https://www.statefarm.com/\", depth=0),\n",
        "]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ltkzbm2nMaDN",
        "outputId": "844ebddc-b5d6-44af-82bd-b23e21c86ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving:  https://www.statefarm.com/ {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36', 'Accept-Language': 'en-US,en;q=0.5'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_chunks = []\n",
        "# splitter = CharacterTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=0)\n",
        "splitter = CharacterTextSplitter(separator=\" \", chunk_size=248, chunk_overlap=0)\n",
        "for source in sources:\n",
        "    for chunk in splitter.split_text(source.page_content):\n",
        "        source_chunks.append(Document(page_content=chunk, metadata=source.metadata))\n",
        "\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "hf = HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "# search_index = FAISS.from_documents(source_chunks, OpenAIEmbeddings())\n",
        "search_index = FAISS.from_documents(source_chunks, hf)"
      ],
      "metadata": {
        "id": "nHSNg1cfsSYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running into index size limitations for the gpt2 model. Need to find a way to set max tokens. For now, use OpenAI"
      ],
      "metadata": {
        "id": "98GzHpMEbqmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "model_id = \"gpt2\" # gpt2\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "pipe = pipeline(\n",
        "    \"text-generation\", model=model, tokenizer=tokenizer, \n",
        ")\n",
        "hf = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "chain = load_qa_with_sources_chain(OpenAI(temperature=0))\n",
        "# chain = load_qa_with_sources_chain(hf, chain_type=\"stuff\", max_tokens=1024)\n",
        "\n",
        "def print_answer(question):\n",
        "    print(\n",
        "        chain(\n",
        "            {\n",
        "                \"input_documents\": search_index.similarity_search(question, k=4),\n",
        "                \"question\": question,\n",
        "            },\n",
        "            return_only_outputs=False,\n",
        "        )[\"output_text\"]\n",
        "    )"
      ],
      "metadata": {
        "id": "6XUK7Rlwrto6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_answer(\"What is your phone number to call if I have questions?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ_SHC5Nptmk",
        "outputId": "e449763f-9806-42c5-bcf2-5ce1fc88a1b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The phone number to call if you have questions is 800-STATEFARM (800-782-8332).\n",
            "SOURCES: https://www.statefarm.com/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_answer(\"are you hiring?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2dkvUFapx16",
        "outputId": "cf8b6c96-27c5-4a3b-91cc-5dfc459bd670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " No, State Farm is not hiring.\n",
            "SOURCES: https://www.statefarm.com/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_answer(\"How long have you been in business?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ybi6U5PQJ70",
        "outputId": "1b28e61f-af39-4670-e38f-dfca0c978d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " State Farm has been in business for 100 years.\n",
            "SOURCES: https://www.statefarm.com/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_answer(\"What types of insurance do you sell?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZURHlLgs5NQn",
        "outputId": "0d2e9cb6-ac17-485a-9cab-6e0daf0e0513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " State Farm sells auto and home insurance, life and health insurance, investment services, banking options, and additional resources.\n",
            "SOURCES: https://www.statefarm.com/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -alh *.pickle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_TEedQ3OXzF",
        "outputId": "135224e3-a8f9-4e11-d70b-a9f1ce49c4a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 25K Mar 11 01:29 scrape_cache.pickle\n"
          ]
        }
      ]
    }
  ]
}